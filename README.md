# QANTA

## Setup
Qanta can be installed and run in one of two ways
1. Use the included [Packer](https://www.packer.io/) script in `packer/packer.json` to build an
Amazon Machine Image (AMI). This will get used to run Qanta on Amazon Web Services (AWS) Elastic
Compute Cloud (EC2). This is the way we use to develop and improve Qanta so it is the most tested
method. For convenience we also periodically publish new AMIs so you don't have to run the Packer
scripts.
2. Use the scripts which Packer runs to install Qanta on your own machine. We include a brief
overview of where these scripts are and what they do, and welcome any contributions to improving
them. Since they are not our primary way of developing Qanta we don't actively maintain it.

Latest QANTA AMI ID on US-west-1: `ami-a4ca8fc4`

### Dependencies
* Python 3.5
* Apache Spark 1.6.1
* Vowpal Wabbit 8.1.1
* Docker 1.11.1
* All python packages in `packer/requirements.txt`

### Installation
4. Download the Illinois Wikifier code (VERSION 2).  Place the data directory in data/wikifier/data and put the wikifier-3.0-jar-with-dependencies.jar in the lib directory http://cogcomp.cs.illinois.edu/page/software_view/Wikifier and put the config directory in data/wikifier/config

## Environment Variables
The majority of QANTA configuration is done through environment variables. Where possible, these
have been set to sensible defaults.

The simplest way to set this up is to copy the contents of `conf/qb-env.sh.template` and make sure
that the script is executed. For example, in your `~/.bashrc` inserting a line `source qb-env.sh`.

Documentation for what each of these does is in the configuration template

## Running QANTA
QANTA can be run in two modes: batch or streaming. Batch mode is used for training and evaluating
large batches of questions at a time. Running the batch pipeline is managed by
[Spotify Luigi](https://github.com/spotify/luigi). Luigi is a pure python make-like framework for
running data pipelines. The QANTA pipeline is specified in `qanta/pipeline.py`. Below are the
pre-requisites that need to be met before running the pipeline and how to run the pipeline itself.
Eventually any data related targets will be moved to Luigi and leave only compile-like targets in
the makefile

### Prerequisites
Execute the following commands

```bash
# Generate the makefile
$ python3 cli.py makefile

# Run pre-requisites
$ make prereqs
```

Additionally, you must have Apache Spark running at the url specified in the environment variable
`QB_SPARK_MASTER`

### Running Batch Mode

1. Start the Luigi daemon: `luigid --background --address 0.0.0.0`
2. Run the pipeline: `luigi --module qanta.pipeline AllSummaries --workers 30` (change 30 to number
of concurrent tasks to run at a time)
3. Observe pipeline progress at [http://hostname:8082](http://hostname:8082)

To rerun any part of the pipeline it is sufficient to delete the target file generated by the task
you wish to rerun.

### Running Streaming Mode
Again, Apache Spark needs to be running at the url specified in the environment variable
`QB_SPARK_MASTER`.

Streaming mode works by coordinating several processes to predict whether or not to buzz given line
of text (a sentence, partial sentence, paragraph, or anything not containing a new line). The Qanta
server is responsible for:
* Creating a socket then waiting until a connection is established
* The connection is established by starting an Apache Spark streaming job that binds its input to
that socket
* Once the connection is established the Qanta server will start streaming questions to Spark until
its queue is empty.
* Each question is also stored in a PostgreSQL database with a column reserved for Spark's response
* The Qanta server will start to poll the database every 100ms to see if Spark completed all the
questions that were queued.

Spark Streaming will then do the following per input line:
* Read the input from the socket
* Extract all features
* Collect the features, form a Vowpal Wabbit input line, and have VW create predictions. This
requires that VW is running in daemon mode
* Save the output to a PostgreSQL database

Once the outputs are saved in the PostgreSQL database
* The Qanta server reads the results and outputs the desired quantities

With that high-level overview in place, here is how you start the whole system.

1. Start the PostgreSQL database (Docker must be running first): `bin/start-postgres.sh`
2. Start the Vowpal Wabbit daemon: `bin/start-vw-daemon.sh model-file.vw`
(eg `data/models/sentence.16.vw`)
3. Start Qanta server: `python3 cli.py qanta_stream`
4. Start Spark Streaming job: `python3 cli.py spark_stream`

## Output File
In the directory `data/results/` there are a number of files and folders which are the output of a quiz bowl run. Below is a description of what each file contains:

* `sentence.X.full.final`: the answer for a question given the full question text
* `sentence.X.full.buzz`: For each (question, sentence, token), the guess with whether to buzz and the weight
* `sentence.X.full.perf`: For each (question, sentence, token), performance statistics
* `sentence.X.full.pred`: Prediction weights from vowpal wabbit
