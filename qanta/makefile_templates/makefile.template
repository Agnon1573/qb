# -------------------------------------------------------
# This Makefile is automatically generated.  If you want to change the
# system, e.g. to add additional features, then edit the file
# generate_makefile.py instead.
# -------------------------------------------------------
data/deep/glove.840B.300d.txt.gz:
	mkdir -p data/deep
	curl http://www-nlp.stanford.edu/data/glove.840B.300d.txt.gz > $@

data/deep/params: data/deep/glove.840B.300d.txt.gz
	python3 guesser/util/format_dan.py --database={{ QBDB }} --threshold={{ MIN_APPEARANCES }}
	python3 guesser/util/load_embeddings.py
	python3 guesser/dan.py

data/kenlm.binary: extractors/mentions.py
	mkdir -p temp
	python3 extractors/mentions.py --build_lm_data
	lmplz -o 5 < temp/wiki_sent > data/kenlm.arpa
	build_binary data/kenlm.arpa $@
	rm temp/wiki_sent
{% for classifier in CLASSIFIER_FIELDS %}
data/classifier/{{ classifier }}.pkl: util/classifier.py extractors/classifier.py
	mkdir -p data/classifier
	python3 util/classifier.py --attribute={{ classifier }}
{% endfor %}
data/wikifier/data/input: util/wikification.py
	rm -rf $@
	mkdir -p $@
	python3 util/wikification.py

data/wikifier/data/output: data/wikifier/data/input
	rm -rf $@
	mkdir -p $@
	cp lib/wikifier-3.0-jar-with-dependencies.jar data/wikifier/wikifier-3.0-jar-with-dependencies.jar
	cp lib/STAND_ALONE_GUROBI.xml data/wikifier/STAND_ALONE_GUROBI.xml
	(cd data/wikifier && java -Xmx10G -jar wikifier-3.0-jar-with-dependencies.jar -annotateData data/input data/output false STAND_ALONE_GUROBI.xml)

data/ir/whoosh_wiki_{{ MIN_APPEARANCES }}/: util/build_whoosh.py
	rm -rf $@
	mkdir -p $@
	mkdir -p data/wikipedia
	python3 util/build_whoosh.py --min_answers={{ MIN_APPEARANCES }} --whoosh_index=$@ --use_wiki

data/ir/whoosh_qb_{{ MIN_APPEARANCES }}/: util/build_whoosh.py
	rm -rf $@
	mkdir -p $@
	python3 util/build_whoosh.py --whoosh_index=$@ --min_answers={{ MIN_APPEARANCES }} --use_qb

data/ir/whoosh_source_{{ MIN_APPEARANCES }}/: util/build_whoosh.py
	rm -rf $@
	mkdir -p $@
	python3 util/build_whoosh.py --whoosh_index=$@ --min_answers={{ MIN_APPEARANCES }} --use_source

data/guesses.db: extract_features.py data/deep/params
	rm data/temp_guesses.db
	python3 extract_features.py --guesses --ans_limit={{ MIN_APPEARANCES }} --guess_db=data/temp_guesses.db
	cp data/temp_guesses.db $@


clm/clm_wrap.cxx: clm/clm.swig
	swig -c++ -python $<

clm/clm_wrap.o: clm/clm_wrap.cxx
	gcc -O3 `python3-config --includes` -fPIC -c $< -o $@

clm/clm.o: clm/clm.cpp clm/clm.h
	gcc -O3 `python3-config --includes` -fPIC -c $< -o $@

clm/_clm.so: clm/clm.o clm/clm_wrap.o
	g++ -shared `python3-config --ldflags` $^ -o $@

data/lm.txt: clm/lm_wrapper.py clm/_clm.so
	mkdir -p data/wikipedia
	python3 run_clm.py --min_answers={{ MIN_APPEARANCES }}

{% for granularity in GRANULARITIES %}{{ granularity | feature_targets(COMPUTE_OPT_FEATURES) }}{% endfor %} : extract_features.py {% for feature in COMPUTE_OPT_FEATURES %} extractors/{{ feature | base_feat }}.py {{ feature | get_feature_prereqs }}{% endfor %}
    # Note, you may have to split these into 2 runs if out of memory errors occur
	python3 run_spark.py extract_features {{ COMPUTE_OPT_FEATURES | join(' ')}}

extract_compute_features: {% for granularity in GRANULARITIES %}{{ granularity | feature_targets(COMPUTE_OPT_FEATURES) }}{% endfor %}

{% for granularity in GRANULARITIES %}{{ granularity | feature_targets(MEMORY_OPT_FEATURES) }}{% endfor %} : extract_features.py {% for feature in MEMORY_OPT_FEATURES %} extractors/{{ feature | base_feat }}.py {{ feature | get_feature_prereqs }}{% endfor %}
    # Note, you may have to split these into 2 runs if out of memory errors occur
	python3 run_spark.py extract_features --lm-memory {{ MEMORY_OPT_FEATURES | join(' ') }}

extract_memory_features: {% for granularity in GRANULARITIES %}{{ granularity | feature_targets(MEMORY_OPT_FEATURES) }}{% endfor %}

extract_features: extract_compute_features extract_memory_features


{% for fold, granularity, weight in FOLDS | product(GRANULARITIES, NEGATIVE_WEIGHTS) %}data/vw_input/{{ fold }}/{{ granularity }}.{{ weight | int }}.vw_input/_SUCCESS {% endfor %}: {% for granularity in GRANULARITIES %}{{ granularity | feature_targets(FEATURE_NAMES) }}{% endfor %}
	python3 run_spark.py merge_features

spark_merge_features: {% for fold, granularity, weight in FOLDS | product(GRANULARITIES, NEGATIVE_WEIGHTS) %}data/vw_input/{{ fold }}/{{ granularity }}.{{ weight | int }}.vw_input/_SUCCESS {% endfor %}

{% for fold, granularity, weight in FOLDS | product(GRANULARITIES, NEGATIVE_WEIGHTS) %}
data/vw_input/{{ fold }}.{{ granularity }}.{{ weight | int }}.vw_input.gz: data/vw_input/{{ fold }}/{{ granularity }}.{{ weight | int }}.vw_input/_SUCCESS
	rm -f data/vw_input/{{ fold }}.{{ granularity }}.{{ weight | int }}.vw_input.gz
	rm -f data/vw_input/{{ fold }}.{{ granularity }}.{{ weight | int }}.vw_input
	rm -f data/vw_input/{{ fold }}.{{ granularity }}.{{ weight | int }}.meta
	cat data/vw_input/{{ fold }}/{{ granularity}}.{{ weight | int }}.vw_input/* | python3 util/split_feature_meta.py data/vw_input/{{ fold }}.{{ granularity }}.{{ weight | int }}.vw_input data/vw_input/{{ fold }}.{{ granularity }}.{{ weight | int }}.meta
	cat data/vw_input/{{ fold }}.{{ granularity }}.{{ weight | int}}.vw_input | gzip > data/vw_input/{{ fold }}.{{ granularity }}.{{ weight | int }}.vw_input.gz
	rm -f data/vw_input/{{ fold }}.{{ granularity }}.{{ weight | int }}.vw_input
{% endfor %}

merge_features:{% for fold, granularity, weight in FOLDS | product(GRANULARITIES, NEGATIVE_WEIGHTS) %} data/vw_input/{{ fold }}.{{ granularity }}.{{ weight | int }}.vw_input.gz{% endfor %}

{% for _, granularity, weight in [''] | product(GRANULARITIES, NEGATIVE_WEIGHTS) %}
data/models/{{ granularity }}.full.{{ weight | int }}.vw: data/vw_input/dev.{{ granularity }}.{{ weight | int }}.vw_input.gz
	mkdir -p data/models
	vw --compressed -d $< --early_terminate 100 -k -q gt -q ga -b 24 --loss_function logistic -f $@
{% for fold in FOLDS %}
data/results/{{ fold }}/{{ granularity }}.{{ weight | int }}.full.pred: data/vw_input/{{ fold }}.{{ granularity }}.{{ weight | int }}.vw_input.gz data/models/{{ granularity }}.full.{{ weight | int }}.vw
	mkdir -p data/results/{{ fold }}
	vw --compressed -t -d data/vw_input/{{ fold }}.{{ granularity }}.{{ weight | int}}.vw_input.gz -i data/models/{{ granularity }}.full.{{ weight | int }}.vw -p $@

data/results/{{ fold }}/{{ granularity }}.{{ weight | int }}.summary.txt: data/results/{{ fold }}/{{ granularity }}.{{ weight | int }}.full.pred data/vw_input/{{ fold }}.{{ granularity }}.{{ weight | int }}.meta
	python3 qanta/reporting/performance.py data/results/{{ fold }}/{{ granularity }}.{{ weight | int }}.full.pred data/vw_input/{{ fold }}.{{ granularity }}.{{ weight | int }}.meta > $@
{% endfor %}
{% endfor %}


