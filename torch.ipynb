{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple, Optional\n",
    "import shutil\n",
    "import os\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from qanta import logging\n",
    "from qanta.datasets.quiz_bowl import QuizBowlDataset\n",
    "from qanta.preprocess import preprocess_dataset, tokenize_question\n",
    "from qanta.guesser.nn import create_load_embeddings_function, convert_text_to_embeddings_indices, compute_n_classes\n",
    "from qanta.manager import (\n",
    "    BaseLogger, TerminateOnNaN, EarlyStopping, ModelCheckpoint, MaxEpochStopping, TrainingManager)\n",
    "from qanta.guesser.torch.dan import flatten_and_offset, batchify, create_save_model, DanModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "log = logging.get(__name__)\n",
    "PTDAN_WE_TMP = '/tmp/qanta/deep/pt_dan_we.pickle'\n",
    "PTDAN_WE = 'pt_dan_we.pickle'\n",
    "load_embeddings = create_load_embeddings_function(PTDAN_WE_TMP, PTDAN_WE, log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = QuizBowlDataset(1, guesser_train=True)\n",
    "training_data = dataset.training_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_text, y_train, x_test_text, y_test, vocab, class_to_i, i_to_class = preprocess_dataset(\n",
    "    training_data\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-09-19 16:38:33,842 - __main__ - INFO - Loading word embeddings from tmp cache\n"
     ]
    }
   ],
   "source": [
    "embeddings, embedding_lookup = load_embeddings(vocab=vocab, expand_glove=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "183067"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_lookup['UNK']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(183068, 300)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "183067"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(embedding_lookup.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = [convert_text_to_embeddings_indices(q, embedding_lookup) for q in x_train_text]\n",
    "for r in x_train:\n",
    "    if len(r) == 0:\n",
    "        r.append(embedding_lookup['UNK'])\n",
    "\n",
    "x_train = np.array(x_train)\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "x_test = [convert_text_to_embeddings_indices(q, embedding_lookup) for q in x_test_text]\n",
    "for r in x_test:\n",
    "    if len(r) == 0:\n",
    "        r.append(embedding_lookup['UNK'])\n",
    "x_test = np.array(x_test)\n",
    "y_test = np.array(y_test)\n",
    "n_classes = compute_n_classes(training_data[1])\n",
    "i_to_word = {ind: word for word, ind in embedding_lookup.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 512\n",
    "learning_rate = .001\n",
    "non_linearity = 'elu'\n",
    "max_epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_batches_train, t_x_train, t_offset_train, t_y_train = batchify(batch_size, x_train, y_train, truncate=True)\n",
    "n_batches_test, t_x_test, t_offset_test, t_y_test = batchify(batch_size, x_test, y_test, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(model, n_batches, t_x_array, t_offset_array, t_y_array, evaluate=False):\n",
    "    if not evaluate:\n",
    "        random_batch_order = np.random.permutation(n_batches)\n",
    "        t_x_array = t_x_array[random_batch_order]\n",
    "        t_offset_array = t_offset_array[random_batch_order]\n",
    "        t_y_array = t_y_array[random_batch_order]\n",
    "\n",
    "    batch_accuracies = []\n",
    "    batch_losses = []\n",
    "    epoch_start = time.time()\n",
    "    for batch in range(n_batches):\n",
    "        t_x_batch = Variable(t_x_array[batch], volatile=evaluate)\n",
    "        t_offset_batch = Variable(t_offset_array[batch], volatile=evaluate)\n",
    "        t_y_batch = Variable(t_y_array[batch], volatile=evaluate)\n",
    "\n",
    "        model.zero_grad()\n",
    "        out = model(t_x_batch, t_offset_batch)\n",
    "        _, preds = torch.max(out, 1)\n",
    "        accuracy = torch.mean(torch.eq(preds, t_y_batch).float()).data[0]\n",
    "        batch_loss = criterion(out, t_y_batch)\n",
    "        if not evaluate:\n",
    "            batch_loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm(model.parameters(), .25)\n",
    "            optimizer.step()\n",
    "\n",
    "        batch_accuracies.append(accuracy)\n",
    "        batch_losses.append(batch_loss.data[0])\n",
    "\n",
    "    epoch_end = time.time()\n",
    "\n",
    "    return np.array(batch_accuracies), np.array(batch_losses), epoch_end - epoch_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    del model\n",
    "    del optimizer\n",
    "    del criterion\n",
    "except:\n",
    "    pass\n",
    "model = DanModel(embeddings.shape[0], n_classes)\n",
    "model.init_weights(initial_embeddings=embeddings)\n",
    "model.cuda()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "manager = TrainingManager([\n",
    "    BaseLogger(log_func=log.info), TerminateOnNaN(),\n",
    "    EarlyStopping(patience=10, verbose=1), MaxEpochStopping(100),\n",
    "    ModelCheckpoint(create_save_model(model), '/tmp/dan.pt')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "train_acc, train_loss, train_time = run_epoch(\n",
    "    model, n_batches_train,\n",
    "    t_x_train, t_offset_train, t_y_train, evaluate=False\n",
    ")\n",
    "print(train_acc.mean(), train_loss.mean(), train_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "test_acc, test_loss, test_time = run_epoch(\n",
    "    model, n_batches_test,\n",
    "    t_x_test, t_offset_test, t_y_test, evaluate=True\n",
    ")\n",
    "print(test_acc.mean(), test_loss.mean(), test_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "t_x_batch = Variable(t_x_test[2], volatile=False)\n",
    "t_offset_batch = Variable(t_offset_test[2], volatile=False)\n",
    "t_y_batch = Variable(t_y_test[2], volatile=False)\n",
    "out = model(t_x_batch, t_offset_batch)\n",
    "loss = criterion(out, t_y_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_x_batch.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.embeddings(t_x_batch, t_offset_batch).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F.softmax(out).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F.nll_loss(F.log_softmax(out), t_y_batch, size_average=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, preds = torch.max(out, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_y_batch.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_y_test[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_training, reasons = manager.instruct(\n",
    "    train_time, train_loss, train_acc,\n",
    "    test_time, test_loss, test_acc\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stop_training, reasons)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
